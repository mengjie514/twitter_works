{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCIOCSl925nCM0hSMnKq4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengjie514/twitter_works/blob/master/PSIII_IV_(demo).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Collection**"
      ],
      "metadata": {
        "id": "TeRBugg1aS3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8u00a3GaAzo",
        "outputId": "0d04d2d5-1648-43b6-b3b1-26bc6075f543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/PSIV.SP/Raw_Data\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "import json, sched, time, csv\n",
        "\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "# Twitter API\n",
        "s = sched.scheduler(time.time, time.sleep)\n",
        "consumer_key = '' \n",
        "consumer_secret = ''\n",
        "access_token_key = ''\n",
        "access_token_secret = ''\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token_key, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidates= [#\"SenAngusKing\", #\"SenShelby\", #\"TTuberville\", #\"lisamurkowski\", #\"DanSullivan_AK\", #\"SenatorSinema\", #\"CaptMarkKelly\", #\"JohnBoozman\", #\"SenTomCotton\", #\"SenFeinstein\", #\"AlexPadilla4CA\", #\"MichaelBennet\", \n",
        "             #\"Hickenlooper\", #\"SenBlumenthal\", #\"ChrisMurphyCT\", #\"SenatorCarper\", #\"ChrisCoons\", #\"marcorubio\", #\"SenRickScott\", #\"ossoff\", #\"ReverendWarnock\", #\"brianschatz\", #\"maziehirono\", #\"crapoforsenate\", \n",
        "             #\"SenatorRisch\", #\"SenatorDurbin\", #\"SenDuckworth\", #\"ToddYoungIN\", #\"SenatorBraun\", #\"ChuckGrassley\", #\"SenJoniErnst\", #\"JerryMoran\", #\"RogerMarshallMD\", #\"LeaderMcConnell\", #\"RandPaul\", #\"BillCassidy\", \n",
        "             #\"SenJohnKennedy\", #\"SenatorCollins\", #\"SenAngusKing\", #\"SenatorCardin\", #\"ChrisVanHollen\", #\"SenWarren\", #\"SenMarkey\", #\"SenStabenow\", #\"SenGaryPeters\", #\"amyklobuchar\", #\"SenTinaSmith\", #\"SenatorWicker\", \n",
        "             #\"SenHydeSmith\", #\"RoyBlunt\", #\"HawleyMO\", #\"SenatorTester\", #\"SteveDaines\", #\"SenatorFischer\", #\"SenSasse\", #\"SenCortezMasto\", #\"RosenforNevada\", #\"SenatorShaheen\", #\"SenatorHassan\", #\"SenatorMenendez\", #\"CoryBooker\", #\"MartinHeinrich\",\n",
        "             #\"SenatorLujan\", #\"SenSchumer\", #\"SenGillibrand\", #\"SenatorBurr\", #\"SenThomTillis\", #\"SenJohnHoeven\", #\"SenKevinCramer\", #\"SenSherrodBrown\", #\"senrobportman\", #\"JimInhofe\", #\"SenatorLankford\", #\"RonWyden\", #\"SenJeffMerkley\", #\"SenBobCasey\",\n",
        "             #\"SenToomey\", #\"SenJackReed\", #\"SenWhitehouse\", #\"LindseyGrahamSC\", #\"SenatorTimScott\", #\"SenJohnThune\", #\"SenatorRounds\", #\"MarshaBlackburn\", #\"BillHagertyTN\", #\"JohnCornyn\", #\"tedcruz\", #\"SenMikeLee\", #\"MittRomney\", #\"SenatorLeahy\", #\"BernieSanders\", #\"MarkWarner\", #\"timkaine\",\n",
        "             #\"PattyMurray\", #\"SenatorCantwell\", #\"Sen_JoeManchin\", #\"SenCapito\", #\"SenRonJohnson\", #\"SenatorBaldwin\", #\"barrassoforwyo\",#\"SenLummis\"\n",
        "]\n",
        "\n",
        "for person in candidates:\n",
        "    saveFile = open(person+\".json\",'a')    \n",
        "    count = 0\n",
        "   \n",
        "for tweet in tweepy.Cursor(api.user_timeline,\n",
        "                                id=person,\n",
        "                                tweet_mode='extended',\n",
        "                                wait_on_rate_limit = True,\n",
        "                                wait_on_rate_limit_notify = True).items():\n",
        "        saveFile.write(json.dumps(tweet._json))\n",
        "        saveFile.write(\"\\n\")\n",
        "        count += 1\n",
        "        print(person+\" \"+str(count ))\n",
        "   \n",
        "saveFile.close\n",
        "\n",
        "print(person+\" finished\")"
      ],
      "metadata": {
        "id": "LeKkQDVHafYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for index, js in enumerate(json_files):\n",
        "    with open(os.path.join(path_to_json, js)) as json_file:\n",
        "      for line in json_file:\n",
        "        try:\n",
        "            tweet = json.loads(line)\n",
        "            if tweet['full_text'].find('RT @')!=-1:\n",
        "            # drop retweeted tweet\n",
        "                continue\n",
        "            data.append(tweet)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "with open('merge_json.json','w') as fp:\n",
        "    json.dump(data,fp)\n",
        "\n",
        "tweets_data_path=('merge_json.json')\n",
        "tweets_data = []\n",
        "tweets_file = open(tweets_data_path,'r')\n",
        "tweets_data = json.load(tweets_file)\n",
        "type(tweets_data)\n",
        "\n",
        "# convert json to df\n",
        "All_df = pd.DataFrame(tweets_data)\n",
        "\n",
        "# select date in certain period\n",
        "All_df['created_at'] = pd.to_datetime(All_df.created_at).dt.date\n",
        "All_df['created_at'] = pd.to_datetime(All_df.created_at, errors='coerce')\n",
        "#All_df = All_df.loc[(All_df['created_at'] >= '2018-10-02') & (All_df['created_at'] <= '2018-12-30')]\n",
        "\n",
        "# language in English \n",
        "All_df = All_df.loc[All_df['lang'] == 'en']\n",
        "\n",
        "# remove RT and reply tweets\n",
        "All_df = All_df[All_df['in_reply_to_screen_name'].isnull()]\n",
        "\n",
        "# extract elements from dataframe \n",
        "col_to_keep = ['created_at', 'id', 'full_text', 'retweet_count','favorite_count', 'user', 'lang']\n",
        "All_df = All_df[col_to_keep]\n",
        "\n",
        "add_followers = []\n",
        "add_friends = []\n",
        "add_screename = []\n",
        "add_userid = []\n",
        "add_userfa = []\n",
        "\n",
        "for index, row in All_df.iterrows():\n",
        "    fo = (row['user']['followers_count'])\n",
        "    fr = (row['user']['friends_count'])\n",
        "    sc = (row['user']['screen_name'])\n",
        "    ud = (row['user']['id'])\n",
        "    uf = (row['user']['favourites_count'])\n",
        "    \n",
        "    add_followers.append(fo)\n",
        "    add_friends.append(fr)\n",
        "    add_screename.append(sc)\n",
        "    add_userid.append(ud)\n",
        "    add_userfa.append(uf)\n",
        "   \n",
        "All_df['followers_count'] = add_followers\n",
        "All_df['friends_count'] = add_friends\n",
        "All_df['screen_name'] = add_screename\n",
        "All_df['user_id'] = add_userid\n",
        "All_df['favourites_count_user'] = add_userfa\n",
        "\n",
        "del All_df['user']\n",
        "del All_df['lang']\n",
        "\n",
        "#All_df.to_csv('PSIV_SP_Raw.csv')"
      ],
      "metadata": {
        "id": "c4ym9MxvgCDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tweets Pre-processing**"
      ],
      "metadata": {
        "id": "3nOlqBXvcqF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/PSIII\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "df = pd.read_csv('Emo_Full.csv', encoding ='unicode_escape')\n",
        "df = All_df[['created_at', 'full_text', 'retweet_count', 'favorite_count', 'followers_count', 'friends_count', 'screen_name']]\n",
        "\n",
        "# text Cleaning\n",
        "import string\n",
        "from html.parser import HTMLParser\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
        "\n",
        "# hashtags check\n",
        "hash_list = df.full_text.str.extractall(\n",
        "    r'(\\#\\w+)'\n",
        ").reset_index().drop_duplicates(['level_0', 0])[0].value_counts()\n",
        "\n",
        "split_adhere_dic = {\n",
        "    \"weve\": \"we have\", \"Weve\": \"We have\", \"youve\": \"you have\", \"Youve\": \"You have\", \"theyve\": \"they have\", \"Theyve\": \"They have\", \"Ive\": \"I have\", \"youre\": \"you are\", \"Youre\": \"You are\", \"theyre\": \"they are\", \"Theyre\": \"They are\",\n",
        "    \"Im\": \"I am\", \"Ill\": \"I will\", \"Id\": \"I would\", \"hes\": \"he's\", \"Hes\": \"He's\", \"shes\": \"she's\", \"Shes\": \"She's\", \"thats\": \"that is\", \"Thats\": \"That is\", \"thatd\": \"that would\", \"Thatd\": \"That would\", \"theres\": \"there is\", \"Theres\": \"There is\", \n",
        "    \"thered\": \"there would\", \"Thered\": \"There would\", \"heres\": \"here is\", \"Heres\": \"Here is\",\n",
        "    # negation cues\n",
        "    \"isn't\": \"is not\", \"isnt\": \"is not\", \"ain't\": \"are not\", \"aint\": \"are not\", \"aren't\": \"are not\", \"arent\": \"are not\", \"weren't\": \"were not\", \"werent\": \"were not\", \"wasn't\": \"was not\", \"wasnt\": \" was not\", \n",
        "    \"cannot\": \"can not\", \"can't\": \" can not\", \"cant\": \" can not\", \"couldn't\": \"could not\", \"couldnt\": \" could not\", \"wouldn't\": \"would not\", \"wouldnt\": \"would not\", \"don't\": \"do not\", \"dont\": \"do not\", \"doesn't\": \"does not\", \"doesnt\": \"does not\",\n",
        "    \"didn't\": \"did not\", \"didnt\": \"did not\", \"haven't\": \"have not\", \"havent\": \"have not\", \"havnt\": \"have not\", \"hasn't\": \"has not\", \"hasnt\": \"has not\", \"hadn't\": \"had not\", \"hadnt\": \"had not\", \"shouldn't\": \"should not\", \"shouldnt\": \"should not\", \n",
        "    \"won't\": \"will not\", \"wont\": \"will not\", \"mightn't\": \"might not\", \"mightnt\": \"might not\", \"mustn't\": \"must not\", \"mustnt\": \"must not\", \"needn't\": \"need not\", \"neednt\": \"need not\"\n",
        "    }\n",
        "\n",
        "pat1 = r'@[\\w_]+'\n",
        "pat2 = r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
        "pat3 = r'#'\n",
        "pat3b = r'#.'\n",
        "pat4 = r'www.[^ ]+'\n",
        "pat5 = r'\\n'\n",
        "pat6= r'\\xa0'\n",
        "pat7= r'AT_USER'\n",
        "pat8 = r'pic.twitter.com/(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
        "pat9 = r'.twitter.com/(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
        "combined_pat = r'|'.join((pat1, pat2, pat3, pat3b, pat4, pat5, pat6, pat7, pat8, pat9))\n",
        "\n",
        "split_pattern = re.compile(r'\\b(' + '|'.join(split_adhere_dic.keys()) + r')\\b')\n",
        "\n",
        "def tweet_cleaner(demo):\n",
        "    soup = BeautifulSoup(demo, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    splitted = re.sub(r\"([a-z\\.!?])([A-Z])\", r\"\\1 \\2\", stripped) #split joined words\n",
        "    split_handled = split_pattern.sub(lambda x: split_adhere_dic[x.group()], splitted)\n",
        "\n",
        "    return split_handled\n",
        "\n",
        "df['new_clean_text'] = [tweet_cleaner(t) for t in df.full_text]\n",
        "\n",
        "# remove stopwords/punctuations/numbers\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# add words that aren't in the NLTK stopwords list and are frequently used in MFT\n",
        "new_stopwords = ['could', 'would', 'day', 'days', 'month', 'months', 'year', 'years', \n",
        "                 'say', 'says', 'said', 'saying', 'gotta', 'wanna', 'etc'] \n",
        "new_stopwords_list = stop_words.union(new_stopwords)\n",
        "\n",
        "# negation Scope Detection for Twitter Sentiment Analysis (Reitan et al, 2015)\n",
        "not_stopwords = {'no', 'not', 'nor', 'none', 'neither', 'never', 'nothing'}\n",
        "final_stop_words = set([word for word in new_stopwords_list if word not in not_stopwords])\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def text_process(deep):\n",
        "    words = re.split(r'\\W+', deep)\n",
        "    nopunc = [w.translate(table) for w in words]\n",
        "    nostop =  ' '.join([word for word in nopunc if word.lower() not in final_stop_words])\n",
        "    nonum = ' '.join([word for word in str(nostop).split() if not word.isdigit()])\n",
        "    return nonum\n",
        "\n",
        "df['new_clean_text_deep'] = df.apply(lambda row: text_process(row.new_clean_text), axis=1)\n",
        "\n",
        "# stemming words\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stem_sentences(sentence):\n",
        "    tokens = sentence.split()\n",
        "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "df['new_clean_text_deep_stem'] = df['new_clean_text_deep'].apply(stem_sentences)"
      ],
      "metadata": {
        "id": "3KLuJxOVuGdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We normalized tweets to:\n",
        "\n",
        "*   convert all alphabets to lowercase to ensure consistency in the text analysis\n",
        "*   remove noisy symbols (e.g., http://, @user), unnecessary digits, punctuation marks, and stop words (e.g., “a”, “the”) that provide the least value in analysis\n",
        "*   replace any words containing redundant letters (e.g., “suppoort”) with the correct spellings (e.g., “support”)\n",
        "*   stem the words (e.g., transferring the inflectional form of “abandoned” to the root form “abandon”) and tokenize the remaining text into the usual format"
      ],
      "metadata": {
        "id": "Xa5mmZe42W3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['full_text', 'new_clean_text_deep_stem', 'screen_name', 'retweet_count', 'favorite_count']].tail(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "bPbpSI3Yuybc",
        "outputId": "474a13b5-ad02-48f3-eea3-3f00fd65d667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                               full_text  \\\n",
              "14584  Great to visit with @CBPWestTexas today in El Paso  where the legend began.' We all owe a debt of gratitude to these gents for the sacrifices they make to uphold our laws, secure our borders, and protect our safety. https://t.co/PMQqSHOLi6   \n",
              "14585  Thanks to the relentless work of @POTUS and Republicans in Congress, TAXES ARE GOING DOWN AND THE WALL IS GOING UP! #MAGA ?? https://t.co/ZJUrCChtkk                                                                                                \n",
              "14586  Few things are more fundamental to a nation than a protected border. Proud to introduce the Build the Wall, Enforce the Law Act. #MAGA ?? https://t.co/1nLHgwUvoV                                                                                   \n",
              "\n",
              "                                                                                                 new_clean_text_deep_stem  \\\n",
              "14584  great visit today el paso legend began owe debt gratitud gent sacrific make uphold law secur border protect safeti   \n",
              "14585  thank relentless work republican congress tax go wall go maga                                                        \n",
              "14586  thing fundament nation protect border proud introduc build wall enforc law act maga                                  \n",
              "\n",
              "      screen_name  retweet_count  favorite_count  \n",
              "14584  GOPLeader   118            291             \n",
              "14585  GOPLeader   4935           13519           \n",
              "14586  GOPLeader   2122           5059            "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c53f3b07-6018-4cf6-a5ee-baca0ad886f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "      <th>new_clean_text_deep_stem</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14584</th>\n",
              "      <td>Great to visit with @CBPWestTexas today in El Paso  where the legend began.' We all owe a debt of gratitude to these gents for the sacrifices they make to uphold our laws, secure our borders, and protect our safety. https://t.co/PMQqSHOLi6</td>\n",
              "      <td>great visit today el paso legend began owe debt gratitud gent sacrific make uphold law secur border protect safeti</td>\n",
              "      <td>GOPLeader</td>\n",
              "      <td>118</td>\n",
              "      <td>291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14585</th>\n",
              "      <td>Thanks to the relentless work of @POTUS and Republicans in Congress, TAXES ARE GOING DOWN AND THE WALL IS GOING UP! #MAGA ?? https://t.co/ZJUrCChtkk</td>\n",
              "      <td>thank relentless work republican congress tax go wall go maga</td>\n",
              "      <td>GOPLeader</td>\n",
              "      <td>4935</td>\n",
              "      <td>13519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14586</th>\n",
              "      <td>Few things are more fundamental to a nation than a protected border. Proud to introduce the Build the Wall, Enforce the Law Act. #MAGA ?? https://t.co/1nLHgwUvoV</td>\n",
              "      <td>thing fundament nation protect border proud introduc build wall enforc law act maga</td>\n",
              "      <td>GOPLeader</td>\n",
              "      <td>2122</td>\n",
              "      <td>5059</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c53f3b07-6018-4cf6-a5ee-baca0ad886f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c53f3b07-6018-4cf6-a5ee-baca0ad886f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c53f3b07-6018-4cf6-a5ee-baca0ad886f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}